# Hiring Assignment: Video Analysis Agent

## Objective
Build an analysis agent that verifies whether a Hercules test run was executed as planned by comparing:
- The agent's planning log (thoughts/steps)
- The video evidence of the run
- The final test output

The agent should flag any claimed action that is skipped, altered, or not visibly performed.

## Inputs
- One or more video recordings of the test run
- A planning log representing the agent's internal steps or reasoning
- The final test output generated by Hercules

## Hercules Artifact Parsing (Local Example)
The sample package in `supportingLogs (1) (1)/` matches a typical Hercules output bundle:
- `test_result.xml` (JUnit): test name, status, failure message, and embedded properties such as `plan`, `next_step`, and proof paths.
- `agent_inner_logs.json`: structured planner history; `planner_agent[*].content` includes `plan`, `next_step`, and `next_step_summary`.
- `video.webm`: proof video for UI actions.
- `test_result.html`: human-readable report (optional).

Minimum parse targets:
- From `test_result.xml`: `testsuite/@name`, `testcase/@name`, `<failure/@message>`, `<property name="plan">`, `<property name="next_step">`, `<property name="Proofs Video">`.
- From `agent_inner_logs.json`: ordered plan steps, summaries, and per-step assertions.

## Processing Steps
1. Parse the planning log to extract intended step-by-step actions.
2. Inspect the video(s) to determine whether each claimed action is visibly executed.
3. Cross-check with the final test output to validate consistency and outcome alignment.

## Atomic Pipeline for Experimentation
1. Discover artifacts: enumerate files, validate presence, and record missing items. Tradeoff: strict validation blocks runs vs. permissive mode that tolerates missing video or logs.
2. Parse `test_result.xml`: extract test metadata, failure message, and embedded plan/next_step properties. Tradeoff: XML-only parsing is fast but may miss richer context in JSON logs.
3. Parse `agent_inner_logs.json`: extract all `plan` strings, split numbered lines into atomic steps, preserve order. Tradeoff: simple split is deterministic but fails on unnumbered or nested steps.
4. Normalize steps: convert each step to `{verb, target, expected_state}` (e.g., `click`, `Search icon`, `search bar visible`). Tradeoff: rule-based normalization is transparent but brittle; LLM parsing is robust but non-deterministic.
5. Video indexing: sample frames at a baseline FPS, compute timestamps, and run OCR. Tradeoff: lower FPS is cheaper but can miss short actions.
6. Evidence extraction: detect UI cues (text labels, search field focus, filter toggles) and track cursor/click changes. Tradeoff: OCR-only is simpler; UI detection improves recall but adds model complexity.
7. Step-to-evidence matching: align each atomic step with candidate time windows; score matches and record supporting frames. Tradeoff: strict matching reduces false positives but increases false negatives.
8. Cross-check with final output: reconcile observed steps with `<failure>` details (e.g., missing "Turtle Neck" filter implies deviation at filter selection). Tradeoff: treating output as ground truth can mask video evidence of unexpected behavior.
9. Deviation report: for each step, output Observed/Deviation with timestamps and evidence notes. Tradeoff: binary results are clearer; confidence scores add nuance but complicate review.

## Expected Output
Produce a deviation report that lists each claimed step and its status.

Example format:

```
Step | Description | Result | Notes
Click "Login" | Click "Login" button | Observed | -
Enter Password | Type password | Deviation | Step skipped in video from 00:21
Submit Form | Submit the form | Observed | -
```

If everything matches, output:
```
No deviations detected.
```

## Requirements
- Language/Framework: Your choice (Python, TypeScript, etc.).
- Orchestration frameworks allowed (AutoGen, LangChain, Haystack, or custom).
- Accuracy is more important than performance.
- Transparency: Explain decisions in the README and video.

## Deliverables
1. GitHub repository containing:
   - Source code for the analysis agent
   - README with:
     - How to run the agent
     - Where output is saved
     - Sample inputs and expected outputs
2. Video walkthrough (screen recording):
   - Explain the approach: input ingestion -> step comparison -> report generation
   - Show a live run on sample Hercules artifacts
   - Display the final deviation report

## Evaluation Criteria
- Correctness: accurately detects observed vs. deviated actions
- Thought process: clear architecture and methodology
- Code quality: clean, readable, modular
- Documentation: easy-to-follow README and instructions
- Usability: drop files in and get a report out

## Reference
- Hercules GitHub: https://github.com/test-zeus-ai/testzeus-hercules
